{
  
    
        "post0": {
            "title": "PyTorch Tutorial - Part 6",
            "content": "Putting it together with optimization! . The first cell is just code from the previous examples along . import torch from torch import nn from torch.utils.data import DataLoader from torchvision import datasets from torchvision.transforms import ToTensor, Lambda training_data = datasets.FashionMNIST( root=&quot;data&quot;, train=True, download=True, transform=ToTensor() ) test_data = datasets.FashionMNIST( root=&quot;data&quot;, train=False, download=True, transform=ToTensor() ) train_dataloader = DataLoader(training_data, batch_size=64) test_dataloader = DataLoader(test_data, batch_size=64) class NeuralNetwork(nn.Module): def __init__(self): super(NeuralNetwork, self).__init__() self.flatten = nn.Flatten() self.linear_relu_stack = nn.Sequential( nn.Linear(28*28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10), ) def forward(self, x): x = self.flatten(x) logits = self.linear_relu_stack(x) return logits model = NeuralNetwork() . Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw . learning_rate = 1e-3 batch_size = 64 epochs = 5 loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) . Training loop . Now we have to setup a training loop! We need to explicitly zero out the gradients on each iteration. We then need to backpropagate the prediction loss using loss.backward() and then just step using optimizer.step(). . def train_loop(dataloader, model, loss_fn, optimizer): size = len(dataloader.dataset) for batch, (X, y) in enumerate(dataloader): # Batch access cause we wrapped the dset with dataloader pred = model(X) loss = loss_fn(pred, y) optimizer.zero_grad loss.backward() optimizer.step() if batch % 100 == 0: loss, current = loss.item(), batch * len(X) print(f&quot;loss: {loss:&gt;7f} [{current:&gt;5d}/{size:&gt;5d}]&quot;) . def test_loop(dataloader, model, loss_fn): size = len(dataloader.dataset) num_batches = len(dataloader) test_loss, correct = 0, 0 with torch.no_grad(): for X, y in dataloader: pred = model(X) test_loss += loss_fn(pred, y).item() correct += (pred.argmax(1) == y).type(torch.float).sum().item() # Get argmax along column since rows are the batch inputs test_loss /= num_batches correct /= size print(f&quot;Test Error: n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} n&quot;) . for t in range(epochs): print(f&quot;Epoch {t+1} n-&quot;) train_loop(train_dataloader, model, loss_fn, optimizer) test_loop(test_dataloader, model, loss_fn) print(&quot;Done!&quot;) . Epoch 1 - loss: 5.049222 [ 0/60000] loss: 4.444196 [ 6400/60000] loss: 4.470216 [12800/60000] loss: 3.558834 [19200/60000] loss: 4.334649 [25600/60000] loss: 4.048509 [32000/60000] loss: 4.656027 [38400/60000] loss: 4.429307 [44800/60000] loss: 3.571862 [51200/60000] loss: 4.724256 [57600/60000] Test Error: Accuracy: 10.0%, Avg loss: 4.471746 Epoch 2 - loss: 4.211938 [ 0/60000] loss: 4.383227 [ 6400/60000] loss: 3.944262 [12800/60000] loss: 3.761158 [19200/60000] loss: 4.501288 [25600/60000] loss: 4.550036 [32000/60000] loss: 3.987212 [38400/60000] loss: 5.760640 [44800/60000] loss: 4.641942 [51200/60000] loss: 5.218049 [57600/60000] Test Error: Accuracy: 10.0%, Avg loss: 4.507261 Epoch 3 - loss: 3.803559 [ 0/60000] loss: 4.146823 [ 6400/60000] loss: 3.874949 [12800/60000] . model = torch.save(model, &#39;model.pth&#39;) .",
            "url": "https://wolfffff.github.io/blog/pytorch/jupyter/2022/04/30/pytorch_6.html",
            "relUrl": "/pytorch/jupyter/2022/04/30/pytorch_6.html",
            "date": " • Apr 30, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "PyTorch Tutorial - Part 5",
            "content": "Autograd . To train, we need access to the gradient of the loss function. To compute this gradient, PyTorch uses torch.autograd which can compute this gradient for any computational graph. . Take the single layer neural network given here: . import torch x = torch.ones(5) # input tensor y = torch.zeros(3) # expected output w = torch.randn(5, 3, requires_grad=True) b = torch.randn(3, requires_grad=True) z = torch.matmul(x, w)+b loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y) loss . tensor(0.5999, grad_fn=&lt;BinaryCrossEntropyWithLogitsBackward0&gt;) . print(f&quot;Gradient function for z = {z.grad_fn}&quot;) print(f&quot;Gradient function for loss = {loss.grad_fn}&quot;) . Gradient function for z = &lt;AddBackward0 object at 0x7f4122fa6650&gt; Gradient function for loss = &lt;BinaryCrossEntropyWithLogitsBackward0 object at 0x7f4122fa6150&gt; . # It&#39;s just a computational graph! loss.backward() print(w.grad) print(b.grad) . tensor([[0.0142, 0.1601, 0.2225], [0.0142, 0.1601, 0.2225], [0.0142, 0.1601, 0.2225], [0.0142, 0.1601, 0.2225], [0.0142, 0.1601, 0.2225]]) tensor([0.0142, 0.1601, 0.2225]) . z = torch.matmul(x, w)+b print(z.requires_grad) with torch.no_grad(): z = torch.matmul(x, w)+b print(z.requires_grad) # We can also do z_det = z.detach() print(z_det.requires_grad) . True False False .",
            "url": "https://wolfffff.github.io/blog/pytorch/jupyter/2022/04/30/pytorch_5.html",
            "relUrl": "/pytorch/jupyter/2022/04/30/pytorch_5.html",
            "date": " • Apr 30, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Building a model!",
            "content": "import os import torch from torch import nn from torch.utils.data import DataLoader from torchvision import datasets, transforms . device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; print(f&quot;Using {device} device&quot;) . Using cuda device . Defining our model class . Here we just need to implement __init__ to actually build the model and __forward__ which actually implements the model. . class NeuralNetwork(nn.Module): def __init__(self): super(NeuralNetwork, self).__init__() # Call nn.Module init! self.flatten = nn.Flatten() # Smash the picture to 1d! self.linear_relu_stack = nn.Sequential( nn.Linear(28*28, 512), # Hidden! Input length is just 28x28 because that&#39;s the image size! For now, we&#39;ll just make the output size 512. nn.ReLU(), nn.Linear(512,512), # Hidden! nn.ReLU(), nn.Linear(512,10) # our 512 vec and put it into our output class size(10)! ) def forward(self, x): x = self.flatten(x) logits = self.linear_relu_stack(x) return logits model = NeuralNetwork().to(device) print(model) . NeuralNetwork( (flatten): Flatten(start_dim=1, end_dim=-1) (linear_relu_stack): Sequential( (0): Linear(in_features=784, out_features=512, bias=True) (1): ReLU() (2): Linear(in_features=512, out_features=512, bias=True) (3): ReLU() (4): Linear(in_features=512, out_features=10, bias=True) ) ) . X = torch.rand(1, 28, 28, device=device) logits = model(X) pred_probab = nn.Softmax(dim=1)(logits) y_pred = pred_probab.argmax(1) print(f&quot;Predicted class: {y_pred}&quot;) . Predicted class: tensor([3], device=&#39;cuda:0&#39;) . print(f&quot;Model structure: {model} n n&quot;) for name, param in model.named_parameters(): print(f&quot;Layer: {name} | Size: {param.size()} | Values : {param[:2]} n&quot;) . Model structure: NeuralNetwork( (flatten): Flatten(start_dim=1, end_dim=-1) (linear_relu_stack): Sequential( (0): Linear(in_features=784, out_features=512, bias=True) (1): ReLU() (2): Linear(in_features=512, out_features=512, bias=True) (3): ReLU() (4): Linear(in_features=512, out_features=10, bias=True) ) ) Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0255, -0.0169, -0.0031, ..., 0.0331, -0.0003, 0.0136], [ 0.0226, -0.0092, -0.0174, ..., -0.0345, 0.0264, -0.0332]], device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;) Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0015, -0.0132], device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;) Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0185, 0.0232, 0.0248, ..., -0.0187, -0.0252, -0.0192], [ 0.0440, 0.0210, -0.0330, ..., -0.0280, -0.0106, -0.0195]], device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;) Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0126, 0.0328], device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;) Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0285, 0.0097, -0.0416, ..., -0.0394, 0.0214, 0.0066], [-0.0118, 0.0147, 0.0257, ..., -0.0321, 0.0127, 0.0089]], device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;) Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0083, -0.0421], device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;) .",
            "url": "https://wolfffff.github.io/blog/2022/04/30/pytorch_4.html",
            "relUrl": "/2022/04/30/pytorch_4.html",
            "date": " • Apr 30, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "PyTorch Tutorial - Part 3",
            "content": "Transforms! . Of course, nearly all of the data we want to use for machine learning doesnt come nicely processed. To make our data suitable for training, we need to implement transforms to modify our data. As shown in the previous tutorial, Datasets have two sets of transforms, transform to modify features and target_transform to modify the labels. . import torch from torchvision import datasets from torchvision.transforms import ToTensor, Lambda ds = datasets.FashionMNIST( root=&quot;data&quot;, train=True, download=True, transform=ToTensor(), target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)) ) # Note the scatter_ which replaces the y-th index with 1 in a 10 length tensor of 0. . Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw .",
            "url": "https://wolfffff.github.io/blog/pytorch/jupyter/2022/04/30/pytorch_3.html",
            "relUrl": "/pytorch/jupyter/2022/04/30/pytorch_3.html",
            "date": " • Apr 30, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "PyTorch Tutorial - Part 2",
            "content": "Datasets and DataLoaders . Now we&#39;ve got tensors down -- lets shift over to building datasets. PyTorch gives two main primitives (DataLoader and Dataset). DataLoader wraps an iterable around Dataset to allow easy access. . Here, we&#39;re going to use a pre-loaded dataset from PyTorch, Fashion-MNIST, that subclasses Dataset. . import torch from torch.utils.data import Dataset from torchvision import datasets from torchvision.transforms import ToTensor import matplotlib.pyplot as plt training_data = datasets.FashionMNIST( root=&quot;data&quot;, train=True, download=True, transform=ToTensor() ) test_data = datasets.FashionMNIST( root=&quot;data&quot;, train=False, download=True, transform=ToTensor() ) . labels_map = { 0: &quot;T-Shirt&quot;, 1: &quot;Trouser&quot;, 2: &quot;Pullover&quot;, 3: &quot;Dress&quot;, 4: &quot;Coat&quot;, 5: &quot;Sandal&quot;, 6: &quot;Shirt&quot;, 7: &quot;Sneaker&quot;, 8: &quot;Bag&quot;, 9: &quot;Ankle Boot&quot;, } figure = plt.figure(figsize=(8, 8)) cols, rows = 3, 3 for i in range(1, cols * rows + 1): sample_idx = torch.randint(len(training_data), size=(1,)).item() img, label = training_data[sample_idx] figure.add_subplot(rows, cols, i) plt.title(labels_map[label]) plt.axis(&quot;off&quot;) plt.imshow(img.squeeze(), cmap=&quot;gray&quot;) plt.show() . But what about loading custom data? . To do this, we can create a custom Dataset class that implements __init__, __len__, and __getitem__. . Lets implement one for a set of images in a folder where we have a file containing the names and an associated label. . import os import pandas as pd from torchvision.io import read_image class CustomImageDataset(Dataset): def __init__(self, annotations_file, img_dir, transform = None, target_transform = None): self.img_labels = pd.read_csv(annotations_file) self.img_dir = img_dir self.transform = transform self.target_transform = target_transform def __len__(self): return len(self.img_labels) def __getitem__(self, idx): img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx,0]) image = read_image(img_path) label = self.img_labels.iloc[idx,1] if self.transform: image = self.transform(image) if self.target_transform: label = self.target_transform(label) return image, label . DataLoaders! . Now lets go back to the MNIST datasets we created before and setup a dataloader for them. DataLoaders append an iterator and allow us to shuffle, batch, and increase data retrieval speed through multiprocessing. . from torch.utils.data import DataLoader train_dataloader = DataLoader(training_data, batch_size = 64, shuffle = True) test_dataloader = DataLoader(test_data, batch_size = 64, shuffle = True) . train_features, train_labels = next(iter(train_dataloader)) print(f&quot;Feature batch shape: {train_features.size()}&quot;) print(f&quot;Labels batch shape: {train_labels.size()}&quot;) img = train_features[0].squeeze() label = train_labels[0] plt.imshow(img, cmap=&quot;gray&quot;) plt.show() print(f&quot;Label: {label}&quot;) . Feature batch shape: torch.Size([64, 1, 28, 28]) Labels batch shape: torch.Size([64]) . Label: 0 .",
            "url": "https://wolfffff.github.io/blog/pytorch/jupyter/2022/04/30/pytorch_2.html",
            "relUrl": "/pytorch/jupyter/2022/04/30/pytorch_2.html",
            "date": " • Apr 30, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "PyTorch Tutorial - Part 1",
            "content": "Tensors! . As I transition from TensorFlow and Keras to PyTorch, I am going back through the basic docs of PyTorch and this series of posts will document that. Nearly all of this is directly from the PyTorch tutorials but rewriting and commenting will hopefully help me learn and possibly help others make the TF -&gt; PyTorch transition cleaner! . Building tensors! Just like with most deep learning frameworks, PyTorch uses tensors to encode the inputs and outputs of models along with the model parameters. The main difference between the n-dimensional arrays of NumPy (ndarrays) is that they&#39;re compatible with hardware accelerators and optimized for automatic differentiation. . import torch import numpy as np . data = [[1,2],[3,4]] data_tensor = torch.tensor(data) # This also works from numpy arrays! data_np = np.array(data) data_tensor_from_np = torch.from_numpy(data_np) # Or even from another tensor! ones_tensor = torch.ones_like(data_tensor) print(f&quot;Ones tensor: n {ones_tensor} n&quot;) rand_tensor = torch.rand_like(data_tensor, dtype = torch.float) # dtype required! print(f&quot;Random tensor: n {rand_tensor} n&quot;) . Ones tensor: tensor([[1, 1], [1, 1]]) Random tensor: tensor([[0.3120, 0.7831], [0.9336, 0.7780]]) . shape = (2,3) rand_tensor = torch.rand(shape) ones_tensor = torch.ones(shape) zeros_tensor = torch.zeros(shape) print(f&quot;Random Tensor: n {rand_tensor} n&quot;) print(f&quot;Ones Tensor: n {ones_tensor} n&quot;) print(f&quot;Zeros Tensor: n {zeros_tensor}&quot;) . Random Tensor: tensor([[0.0923, 0.6174, 0.4717], [0.7022, 0.7585, 0.7783]]) Ones Tensor: tensor([[1., 1., 1.], [1., 1., 1.]]) Zeros Tensor: tensor([[0., 0., 0.], [0., 0., 0.]]) . print(f&quot;Shape of tensor: {rand_tensor.shape}&quot;) print(f&quot;Datatype of tensor: {rand_tensor.dtype}&quot;) print(f&quot;Device tensor is stored on: {rand_tensor.device}&quot;) . Shape of tensor: torch.Size([2, 3]) Datatype of tensor: torch.float32 Device tensor is stored on: cpu . Operations . Now lets quickly move into operations on tensors. Because we&#39;re working on Colab, we&#39;re gonna go ahead and move it to the GPU for speed! . if torch.cuda.is_available(): print(&quot;Cuda is available -- moving the tensor to the GPU!&quot;) tensor=tensor.to(&quot;cuda&quot;) . Cuda is available -- moving the tensor to the GPU! . tensor = torch.ones(4, 4) print(f&quot;First row: {tensor[0]}&quot;) print(f&quot;First column: {tensor[:, 0]}&quot;) print(f&quot;Last column: {tensor[..., -1]}&quot;) tensor[:,1] = 0 print(tensor) . First row: tensor([1., 1., 1., 1.]) First column: tensor([1., 1., 1., 1.]) Last column: tensor([1., 1., 1., 1.]) tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]]) . # Here we&#39;re just appending along dim 1 (cols) print(torch.cat([tensor,tensor,tensor],dim=1)) . tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.], [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.], [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.], [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]]) . Now onto some actual computations. Lets start with some basic operations. . # Note that for y3, we&#39;re just filling a preallocated tensor with the product. y1 = tensor @ tensor.T y2 = tensor.matmul(tensor.T) y3 = torch.rand_like(tensor) torch.matmul(tensor, tensor.T, out=y3) . tensor([[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) . z1 = tensor * tensor z2 = tensor.mul(tensor) z3 = torch.rand_like(tensor) torch.mul(tensor,tensor,out=z3) . tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]]) . sum = tensor.sum() print(sum.item(), type(sum.item())) . 12.0 &lt;class &#39;float&#39;&gt; . print(f&quot;{tensor} n&quot;) tensor.add_(5) # Just leave off the _ to stop this from being in-place! print(f&quot;{tensor}&quot;) . tensor([[6., 5., 6., 6.], [6., 5., 6., 6.], [6., 5., 6., 6.], [6., 5., 6., 6.]]) tensor([[11., 10., 11., 11.], [11., 10., 11., 11.], [11., 10., 11., 11.], [11., 10., 11., 11.]]) . NumPy bridging! . We can share the underlying memory between tensors and numpy arrays when on the cpu. . t = torch.ones(5) print(f&quot;t: {t}&quot;) n = t.numpy() print(f&quot;n: {n}&quot;) t[2] = 123 print(f&quot;t: {t}&quot;) print(f&quot;n: {n}&quot;) . t: tensor([1., 1., 1., 1., 1.]) n: [1. 1. 1. 1. 1.] t: tensor([ 1., 1., 123., 1., 1.]) n: [ 1. 1. 123. 1. 1.] . n = np.ones(5) print(f&quot;n: {n}&quot;) t = torch.from_numpy(n) print(f&quot;t: {t}&quot;) n[2] = 123 print(f&quot;n: {n}&quot;) print(f&quot;t: {t}&quot;) . n: [1. 1. 1. 1. 1.] t: tensor([1., 1., 1., 1., 1.], dtype=torch.float64) n: [ 1. 1. 123. 1. 1.] t: tensor([ 1., 1., 123., 1., 1.], dtype=torch.float64) .",
            "url": "https://wolfffff.github.io/blog/pytorch/jupyter/2022/04/30/pytorch_1.html",
            "relUrl": "/pytorch/jupyter/2022/04/30/pytorch_1.html",
            "date": " • Apr 30, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://wolfffff.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://wolfffff.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}