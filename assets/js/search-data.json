{
  
    
        "post0": {
            "title": "A simple notebook create a neural collaborative filtering recommendation system",
            "content": "from google.colab import drive drive.mount(&quot;/content/drive/&quot;) # Create this dir as needed/if you want %cd /content/drive/My Drive/Colab Notebooks/data/movielens . Mounted at /content/drive/ /content/drive/My Drive/Colab Notebooks/data/movielens . %load_ext tensorboard from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter() # %%capture # !pip install wandb # import wandb # wandb.init() . import numpy as np import pandas as pd import matplotlib.pyplot as plt import torch from torch import nn from torch import optim from torch.utils.data import DataLoader, Dataset from torchvision.transforms import ToTensor if torch.cuda.is_available(): dev = &quot;cuda&quot; else: dev = &quot;cpu&quot; . # !unzip -j ml-latest.zip # !wget https://files.grouplens.org/datasets/movielens/ml-latest-small.zip # !unzip -j ml-latest-small.zip # movies_df = pd.read_csv(&quot;movies.csv&quot;) # movies_df = movies_df.join(movies_df.pop(&#39;genres&#39;).str.get_dummies(&#39;|&#39;)) # movies_df = movies_df.drop([&quot;title&quot;], axis=1) ratings_df = pd.read_csv(&quot;ratings.csv&quot;) . Preprocessing . Lets map the ids of movies and users to 0:#movies and 0:#users respectively. This allows the later embedding step to work properly. . unique_users = ratings_df.userId.unique() user_to_index = {old: new for new, old in enumerate(unique_users)} ratings_df.userId = ratings_df.userId.map(user_to_index) unique_movies = ratings_df.movieId.unique() movie_to_index = {old: new for new, old in enumerate(unique_movies)} ratings_df.movieId = ratings_df.movieId.map(movie_to_index) . Building dataset . Here we make a basic subclass of Dataset to make all of our subsequent data manipulation much easier. We&#39;ll shift to a DataLoader later but that&#39;s super direct once we have a nicely defined dataset class. . class MovieLensDataset(Dataset): def __init__(self, ratings, transform=None, target_transform=None, dev=&quot;cpu&quot;): self.transform = transform self.target_transform = target_transform self.n_users = ratings_df.userId.unique().shape[0] self.n_movies = ratings_df.movieId.unique().shape[0] self.X = ratings[[&quot;userId&quot;, &quot;movieId&quot;]].values self.y = ratings[&quot;rating&quot;].astype(np.float32) if self.transform: self.X = self.transform(self.X) if self.target_transform: self.y = self.target_transform(self.y) def __len__(self): return self.X.shape[0] def __getitem__(self, idx): X = self.X[idx, :] y = self.y[idx] return X, y . ds = MovieLensDataset( ratings_df, transform=torch.LongTensor, target_transform=torch.FloatTensor, dev=dev ) # Split this up using random_split! train_size = int(0.8 * len(ds)) test_size = len(ds) - train_size train_dataset, test_dataset = torch.utils.data.random_split(ds, [train_size, test_size]) . Network definintion . Now lets take advantage of neural collaborative filtering architecture. Here, we have a model that takes embeddings, concatenates them, and then runs a traditional dense network. . Training loop . Setup a basic training and test loop with a scheduler -- we&#39;ll define the parameters below. . from itertools import zip_longest class EmbeddingNet(nn.Module): &quot;&quot;&quot; Creates a dense network with embedding layers. Args: n_users: Number of unique users in the dataset. n_movies: Number of unique movies in the dataset. n_factors: Number of columns in the embeddings matrix. embedding_dropout: Dropout rate to apply right after embeddings layer. hidden: A single integer or a list of integers defining the number of units in hidden layer(s). dropouts: A single integer or a list of integers defining the dropout layers rates applyied right after each of hidden layers. &quot;&quot;&quot; def __init__( self, n_users, n_movies, n_factors=50, embedding_dropout=0.02, hidden=10, dropouts=0.2, ): self.metric = 0 super().__init__() hidden = get_list(hidden) dropouts = get_list(dropouts) n_last = hidden[-1] def gen_layers(n_in): &quot;&quot;&quot; A generator that yields a sequence of hidden layers and their activations/dropouts. Note that the function captures `hidden` and `dropouts` values from the outer scope. &quot;&quot;&quot; nonlocal hidden, dropouts assert len(dropouts) &lt;= len(hidden) for n_out, rate in zip_longest(hidden, dropouts): yield nn.Linear(n_in, n_out) yield nn.ReLU() if rate is not None and rate &gt; 0.0: yield nn.Dropout(rate) n_in = n_out self.u = nn.Embedding(n_users, n_factors) self.m = nn.Embedding(n_movies, n_factors) self.drop = nn.Dropout(embedding_dropout) self.hidden = nn.Sequential(*list(gen_layers(n_factors * 2))) self.fc = nn.Linear(n_last, 1) self._init() def forward(self, user, movie, minmax=None): features = torch.cat([self.u(user), self.m(movie)], dim=1) x = self.drop(features) x = self.hidden(x) out = torch.sigmoid(self.fc(x)) if minmax is not None: min_rating, max_rating = minmax out = out * (max_rating - min_rating + 1) + min_rating - 0.5 return out def _init(self): &quot;&quot;&quot; Setup embeddings and hidden layers with reasonable initial values. &quot;&quot;&quot; def init(m): if type(m) == nn.Linear: torch.nn.init.xavier_uniform_(m.weight) m.bias.data.fill_(0.01) self.u.weight.data.uniform_(-0.05, 0.05) self.m.weight.data.uniform_(-0.05, 0.05) self.hidden.apply(init) init(self.fc) def get_list(n): if isinstance(n, (int, float)): return [n] elif hasattr(n, &quot;__iter__&quot;): return list(n) raise TypeError( &quot;layers configuraiton should be a single number or a list of numbers&quot; ) . Now lets actually create a model! Here we want each embedding to have dim 150 and 3 hidden layers as specified. . def train_loop(dataloader, model, loss_fn, optimizer, scheduler, epoch): size = len(dataloader.dataset) for batch, (X, y) in enumerate(dataloader): X, y = X.to(dev), y.to(dev) optimizer.zero_grad() # Compute prediction and loss pred = model(X[:, 0], X[:, 1], (0.5, 5)) loss = loss_fn(pred, y.view(-1, 1)) # Backpropagation loss.backward() optimizer.step() if batch % 10 == 0: loss = loss.item() print(f&quot;Loss: {loss:&gt;8f}&quot;) writer.add_scalar(&quot;Loss/train&quot;, loss, epoch) scheduler.step(model.metric) def test_loop(dataloader, model, loss_fn, epoch): size = len(dataloader.dataset) num_batches = len(dataloader) test_loss = 0 with torch.no_grad(): for X, y in dataloader: X, y = X.to(dev), y.to(dev) pred = model(X[:, 0], X[:, 1], (0.5, 5)) test_loss += loss_fn(pred, y.view(-1, 1)).item() test_loss /= num_batches print(f&quot;Test avg loss: {test_loss:&gt;8f}&quot;) writer.add_scalar(&quot;Loss/test&quot;, test_loss, epoch) model.metric = test_loss . net = EmbeddingNet( n_users=ds.n_users, n_movies=ds.n_movies, n_factors=150, hidden=[200, 200, 200], embedding_dropout=0.05, dropouts=[0.5, 0.5, 0.25], ) net.to(dev) lr = 1e-3 wd = 1e-5 bs = 1 n_epochs = 10 batch_size = 100000 lr_plateau_threshold = 0.01 criterion = nn.MSELoss() optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd) scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer, &quot;min&quot;, threshold=lr_plateau_threshold ) from torch.utils.data import DataLoader train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True) for epoch in range(n_epochs): print(f&quot;Epoch {epoch+1} n-&quot;) train_loop(train_dataloader, net, criterion, optimizer, scheduler, epoch + 1) test_loop(test_dataloader, net, criterion, epoch + 1) print(&quot;Done!&quot;) . Epoch 1 - Loss: 1.632245 Test avg loss: 1.470778 Epoch 2 - Loss: 1.465155 Test avg loss: 1.333617 Epoch 3 - Loss: 1.326258 Test avg loss: 1.224822 Epoch 4 - Loss: 1.211926 Test avg loss: 1.138123 Epoch 5 - Loss: 1.123828 Test avg loss: 1.090433 Epoch 6 - Loss: 1.067004 Test avg loss: 1.077034 Epoch 7 - Loss: 1.051721 Test avg loss: 1.099112 Epoch 8 - Loss: 1.067773 Test avg loss: 1.113712 Epoch 9 - Loss: 1.080225 Test avg loss: 1.106285 Epoch 10 - Loss: 1.071681 Test avg loss: 1.080715 Done! . writer.flush() writer.close() %tensorboard --logdir runs from tensorboard import notebook notebook.list() # View open TensorBoard instances # Control TensorBoard display. If no port is provided, # the most recently launched TensorBoard is used notebook.display(port=6006, height=1000) . Known TensorBoard instances: - port 6006: logdir runs (started 0:00:01 ago; pid 317) Selecting TensorBoard with logdir runs (started 0:00:01 ago; port 6006, pid 317). . X, y = train_dataset[0] .",
            "url": "https://wolfffff.github.io/fastpages/2022/05/04/_05_02_hybrid_content_collab_recsys.html",
            "relUrl": "/2022/05/04/_05_02_hybrid_content_collab_recsys.html",
            "date": " • May 4, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "PyTorch Tutorial - Part 6",
            "content": "Putting it together with optimization! . The first cell is just code from the previous examples along . import torch from torch import nn from torch.utils.data import DataLoader from torchvision import datasets from torchvision.transforms import ToTensor, Lambda training_data = datasets.FashionMNIST( root=&quot;data&quot;, train=True, download=True, transform=ToTensor() ) test_data = datasets.FashionMNIST( root=&quot;data&quot;, train=False, download=True, transform=ToTensor() ) train_dataloader = DataLoader(training_data, batch_size=64) test_dataloader = DataLoader(test_data, batch_size=64) class NeuralNetwork(nn.Module): def __init__(self): super(NeuralNetwork, self).__init__() self.flatten = nn.Flatten() self.linear_relu_stack = nn.Sequential( nn.Linear(28*28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10), ) def forward(self, x): x = self.flatten(x) logits = self.linear_relu_stack(x) return logits model = NeuralNetwork() . Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw . learning_rate = 1e-3 batch_size = 64 epochs = 5 loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) . Training loop . Now we have to setup a training loop! We need to explicitly zero out the gradients on each iteration. We then need to backpropagate the prediction loss using loss.backward() and then just step using optimizer.step(). . def train_loop(dataloader, model, loss_fn, optimizer): size = len(dataloader.dataset) for batch, (X, y) in enumerate(dataloader): # Batch access cause we wrapped the dset with dataloader pred = model(X) loss = loss_fn(pred, y) optimizer.zero_grad loss.backward() optimizer.step() if batch % 100 == 0: loss, current = loss.item(), batch * len(X) print(f&quot;loss: {loss:&gt;7f} [{current:&gt;5d}/{size:&gt;5d}]&quot;) . def test_loop(dataloader, model, loss_fn): size = len(dataloader.dataset) num_batches = len(dataloader) test_loss, correct = 0, 0 with torch.no_grad(): for X, y in dataloader: pred = model(X) test_loss += loss_fn(pred, y).item() correct += (pred.argmax(1) == y).type(torch.float).sum().item() # Get argmax along column since rows are the batch inputs test_loss /= num_batches correct /= size print(f&quot;Test Error: n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} n&quot;) . for t in range(epochs): print(f&quot;Epoch {t+1} n-&quot;) train_loop(train_dataloader, model, loss_fn, optimizer) test_loop(test_dataloader, model, loss_fn) print(&quot;Done!&quot;) . Epoch 1 - loss: 5.049222 [ 0/60000] loss: 4.444196 [ 6400/60000] loss: 4.470216 [12800/60000] loss: 3.558834 [19200/60000] loss: 4.334649 [25600/60000] loss: 4.048509 [32000/60000] loss: 4.656027 [38400/60000] loss: 4.429307 [44800/60000] loss: 3.571862 [51200/60000] loss: 4.724256 [57600/60000] Test Error: Accuracy: 10.0%, Avg loss: 4.471746 Epoch 2 - loss: 4.211938 [ 0/60000] loss: 4.383227 [ 6400/60000] loss: 3.944262 [12800/60000] loss: 3.761158 [19200/60000] loss: 4.501288 [25600/60000] loss: 4.550036 [32000/60000] loss: 3.987212 [38400/60000] loss: 5.760640 [44800/60000] loss: 4.641942 [51200/60000] loss: 5.218049 [57600/60000] Test Error: Accuracy: 10.0%, Avg loss: 4.507261 Epoch 3 - loss: 3.803559 [ 0/60000] loss: 4.146823 [ 6400/60000] loss: 3.874949 [12800/60000] . model = torch.save(model, &#39;model.pth&#39;) .",
            "url": "https://wolfffff.github.io/fastpages/pytorch/jupyter/2022/04/30/pytorch_6.html",
            "relUrl": "/pytorch/jupyter/2022/04/30/pytorch_6.html",
            "date": " • Apr 30, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "PyTorch Tutorial - Part 5",
            "content": "Autograd . To train, we need access to the gradient of the loss function. To compute this gradient, PyTorch uses torch.autograd which can compute this gradient for any computational graph. . Take the single layer neural network given here: . import torch x = torch.ones(5) # input tensor y = torch.zeros(3) # expected output w = torch.randn(5, 3, requires_grad=True) b = torch.randn(3, requires_grad=True) z = torch.matmul(x, w)+b loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y) loss . tensor(0.5999, grad_fn=&lt;BinaryCrossEntropyWithLogitsBackward0&gt;) . print(f&quot;Gradient function for z = {z.grad_fn}&quot;) print(f&quot;Gradient function for loss = {loss.grad_fn}&quot;) . Gradient function for z = &lt;AddBackward0 object at 0x7f4122fa6650&gt; Gradient function for loss = &lt;BinaryCrossEntropyWithLogitsBackward0 object at 0x7f4122fa6150&gt; . # It&#39;s just a computational graph! loss.backward() print(w.grad) print(b.grad) . tensor([[0.0142, 0.1601, 0.2225], [0.0142, 0.1601, 0.2225], [0.0142, 0.1601, 0.2225], [0.0142, 0.1601, 0.2225], [0.0142, 0.1601, 0.2225]]) tensor([0.0142, 0.1601, 0.2225]) . z = torch.matmul(x, w)+b print(z.requires_grad) with torch.no_grad(): z = torch.matmul(x, w)+b print(z.requires_grad) # We can also do z_det = z.detach() print(z_det.requires_grad) . True False False .",
            "url": "https://wolfffff.github.io/fastpages/pytorch/jupyter/2022/04/30/pytorch_5.html",
            "relUrl": "/pytorch/jupyter/2022/04/30/pytorch_5.html",
            "date": " • Apr 30, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "PyTorch Tutorial - Part 4",
            "content": "Building a model! . Finally -- we can build the model that PyTorch promises. Sure, there have been a few extra steps relative to Keras/TF, but here is where PyTorch shines. . import os import torch from torch import nn from torch.utils.data import DataLoader from torchvision import datasets, transforms . device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; print(f&quot;Using {device} device&quot;) . Using cuda device . Defining our model class . Here we just need to implement __init__ to actually build the model and __forward__ which actually implements the model. . class NeuralNetwork(nn.Module): def __init__(self): super(NeuralNetwork, self).__init__() # Call nn.Module init! self.flatten = nn.Flatten() # Smash the picture to 1d! self.linear_relu_stack = nn.Sequential( nn.Linear(28*28, 512), # Hidden! Input length is just 28x28 because that&#39;s the image size! For now, we&#39;ll just make the output size 512. nn.ReLU(), nn.Linear(512,512), # Hidden! nn.ReLU(), nn.Linear(512,10) # our 512 vec and put it into our output class size(10)! ) def forward(self, x): x = self.flatten(x) logits = self.linear_relu_stack(x) return logits model = NeuralNetwork().to(device) print(model) . NeuralNetwork( (flatten): Flatten(start_dim=1, end_dim=-1) (linear_relu_stack): Sequential( (0): Linear(in_features=784, out_features=512, bias=True) (1): ReLU() (2): Linear(in_features=512, out_features=512, bias=True) (3): ReLU() (4): Linear(in_features=512, out_features=10, bias=True) ) ) . X = torch.rand(1, 28, 28, device=device) logits = model(X) pred_probab = nn.Softmax(dim=1)(logits) y_pred = pred_probab.argmax(1) print(f&quot;Predicted class: {y_pred}&quot;) . Predicted class: tensor([3], device=&#39;cuda:0&#39;) . print(f&quot;Model structure: {model} n n&quot;) for name, param in model.named_parameters(): print(f&quot;Layer: {name} | Size: {param.size()} | Values : {param[:2]} n&quot;) . Model structure: NeuralNetwork( (flatten): Flatten(start_dim=1, end_dim=-1) (linear_relu_stack): Sequential( (0): Linear(in_features=784, out_features=512, bias=True) (1): ReLU() (2): Linear(in_features=512, out_features=512, bias=True) (3): ReLU() (4): Linear(in_features=512, out_features=10, bias=True) ) ) Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0255, -0.0169, -0.0031, ..., 0.0331, -0.0003, 0.0136], [ 0.0226, -0.0092, -0.0174, ..., -0.0345, 0.0264, -0.0332]], device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;) Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0015, -0.0132], device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;) Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0185, 0.0232, 0.0248, ..., -0.0187, -0.0252, -0.0192], [ 0.0440, 0.0210, -0.0330, ..., -0.0280, -0.0106, -0.0195]], device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;) Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0126, 0.0328], device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;) Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0285, 0.0097, -0.0416, ..., -0.0394, 0.0214, 0.0066], [-0.0118, 0.0147, 0.0257, ..., -0.0321, 0.0127, 0.0089]], device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;) Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0083, -0.0421], device=&#39;cuda:0&#39;, grad_fn=&lt;SliceBackward0&gt;) .",
            "url": "https://wolfffff.github.io/fastpages/pytorch/jupyter/2022/04/30/pytorch_4.html",
            "relUrl": "/pytorch/jupyter/2022/04/30/pytorch_4.html",
            "date": " • Apr 30, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "PyTorch Tutorial - Part 3",
            "content": "Transforms! . Of course, nearly all of the data we want to use for machine learning doesnt come nicely processed. To make our data suitable for training, we need to implement transforms to modify our data. As shown in the previous tutorial, Datasets have two sets of transforms, transform to modify features and target_transform to modify the labels. . import torch from torchvision import datasets from torchvision.transforms import ToTensor, Lambda ds = datasets.FashionMNIST( root=&quot;data&quot;, train=True, download=True, transform=ToTensor(), target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)) ) # Note the scatter_ which replaces the y-th index with 1 in a 10 length tensor of 0. . Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw .",
            "url": "https://wolfffff.github.io/fastpages/pytorch/jupyter/2022/04/30/pytorch_3.html",
            "relUrl": "/pytorch/jupyter/2022/04/30/pytorch_3.html",
            "date": " • Apr 30, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "PyTorch Tutorial - Part 2",
            "content": "Datasets and DataLoaders . Now we&#39;ve got tensors down -- lets shift over to building datasets. PyTorch gives two main primitives (DataLoader and Dataset). DataLoader wraps an iterable around Dataset to allow easy access. . Here, we&#39;re going to use a pre-loaded dataset from PyTorch, Fashion-MNIST, that subclasses Dataset. . import torch from torch.utils.data import Dataset from torchvision import datasets from torchvision.transforms import ToTensor import matplotlib.pyplot as plt training_data = datasets.FashionMNIST( root=&quot;data&quot;, train=True, download=True, transform=ToTensor() ) test_data = datasets.FashionMNIST( root=&quot;data&quot;, train=False, download=True, transform=ToTensor() ) . labels_map = { 0: &quot;T-Shirt&quot;, 1: &quot;Trouser&quot;, 2: &quot;Pullover&quot;, 3: &quot;Dress&quot;, 4: &quot;Coat&quot;, 5: &quot;Sandal&quot;, 6: &quot;Shirt&quot;, 7: &quot;Sneaker&quot;, 8: &quot;Bag&quot;, 9: &quot;Ankle Boot&quot;, } figure = plt.figure(figsize=(8, 8)) cols, rows = 3, 3 for i in range(1, cols * rows + 1): sample_idx = torch.randint(len(training_data), size=(1,)).item() img, label = training_data[sample_idx] figure.add_subplot(rows, cols, i) plt.title(labels_map[label]) plt.axis(&quot;off&quot;) plt.imshow(img.squeeze(), cmap=&quot;gray&quot;) plt.show() . But what about loading custom data? . To do this, we can create a custom Dataset class that implements __init__, __len__, and __getitem__. . Lets implement one for a set of images in a folder where we have a file containing the names and an associated label. . import os import pandas as pd from torchvision.io import read_image class CustomImageDataset(Dataset): def __init__(self, annotations_file, img_dir, transform = None, target_transform = None): self.img_labels = pd.read_csv(annotations_file) self.img_dir = img_dir self.transform = transform self.target_transform = target_transform def __len__(self): return len(self.img_labels) def __getitem__(self, idx): img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx,0]) image = read_image(img_path) label = self.img_labels.iloc[idx,1] if self.transform: image = self.transform(image) if self.target_transform: label = self.target_transform(label) return image, label . DataLoaders! . Now lets go back to the MNIST datasets we created before and setup a dataloader for them. DataLoaders append an iterator and allow us to shuffle, batch, and increase data retrieval speed through multiprocessing. . from torch.utils.data import DataLoader train_dataloader = DataLoader(training_data, batch_size = 64, shuffle = True) test_dataloader = DataLoader(test_data, batch_size = 64, shuffle = True) . train_features, train_labels = next(iter(train_dataloader)) print(f&quot;Feature batch shape: {train_features.size()}&quot;) print(f&quot;Labels batch shape: {train_labels.size()}&quot;) img = train_features[0].squeeze() label = train_labels[0] plt.imshow(img, cmap=&quot;gray&quot;) plt.show() print(f&quot;Label: {label}&quot;) . Feature batch shape: torch.Size([64, 1, 28, 28]) Labels batch shape: torch.Size([64]) . Label: 0 .",
            "url": "https://wolfffff.github.io/fastpages/pytorch/jupyter/2022/04/30/pytorch_2.html",
            "relUrl": "/pytorch/jupyter/2022/04/30/pytorch_2.html",
            "date": " • Apr 30, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "PyTorch Tutorial - Part 1",
            "content": "Tensors! . As I transition from TensorFlow and Keras to PyTorch, I am going back through the basic docs of PyTorch and this series of posts will document that. Nearly all of this is directly from the PyTorch tutorials but rewriting and commenting will hopefully help me learn and possibly help others make the TF -&gt; PyTorch transition cleaner! . Building tensors! Just like with most deep learning frameworks, PyTorch uses tensors to encode the inputs and outputs of models along with the model parameters. The main difference between the n-dimensional arrays of NumPy (ndarrays) is that they&#39;re compatible with hardware accelerators and optimized for automatic differentiation. . import torch import numpy as np . data = [[1,2],[3,4]] data_tensor = torch.tensor(data) # This also works from numpy arrays! data_np = np.array(data) data_tensor_from_np = torch.from_numpy(data_np) # Or even from another tensor! ones_tensor = torch.ones_like(data_tensor) print(f&quot;Ones tensor: n {ones_tensor} n&quot;) rand_tensor = torch.rand_like(data_tensor, dtype = torch.float) # dtype required! print(f&quot;Random tensor: n {rand_tensor} n&quot;) . Ones tensor: tensor([[1, 1], [1, 1]]) Random tensor: tensor([[0.3120, 0.7831], [0.9336, 0.7780]]) . shape = (2,3) rand_tensor = torch.rand(shape) ones_tensor = torch.ones(shape) zeros_tensor = torch.zeros(shape) print(f&quot;Random Tensor: n {rand_tensor} n&quot;) print(f&quot;Ones Tensor: n {ones_tensor} n&quot;) print(f&quot;Zeros Tensor: n {zeros_tensor}&quot;) . Random Tensor: tensor([[0.0923, 0.6174, 0.4717], [0.7022, 0.7585, 0.7783]]) Ones Tensor: tensor([[1., 1., 1.], [1., 1., 1.]]) Zeros Tensor: tensor([[0., 0., 0.], [0., 0., 0.]]) . print(f&quot;Shape of tensor: {rand_tensor.shape}&quot;) print(f&quot;Datatype of tensor: {rand_tensor.dtype}&quot;) print(f&quot;Device tensor is stored on: {rand_tensor.device}&quot;) . Shape of tensor: torch.Size([2, 3]) Datatype of tensor: torch.float32 Device tensor is stored on: cpu . Operations . Now lets quickly move into operations on tensors. Because we&#39;re working on Colab, we&#39;re gonna go ahead and move it to the GPU for speed! . if torch.cuda.is_available(): print(&quot;Cuda is available -- moving the tensor to the GPU!&quot;) tensor=tensor.to(&quot;cuda&quot;) . Cuda is available -- moving the tensor to the GPU! . tensor = torch.ones(4, 4) print(f&quot;First row: {tensor[0]}&quot;) print(f&quot;First column: {tensor[:, 0]}&quot;) print(f&quot;Last column: {tensor[..., -1]}&quot;) tensor[:,1] = 0 print(tensor) . First row: tensor([1., 1., 1., 1.]) First column: tensor([1., 1., 1., 1.]) Last column: tensor([1., 1., 1., 1.]) tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]]) . # Here we&#39;re just appending along dim 1 (cols) print(torch.cat([tensor,tensor,tensor],dim=1)) . tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.], [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.], [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.], [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]]) . Now onto some actual computations. Lets start with some basic operations. . # Note that for y3, we&#39;re just filling a preallocated tensor with the product. y1 = tensor @ tensor.T y2 = tensor.matmul(tensor.T) y3 = torch.rand_like(tensor) torch.matmul(tensor, tensor.T, out=y3) . tensor([[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) . z1 = tensor * tensor z2 = tensor.mul(tensor) z3 = torch.rand_like(tensor) torch.mul(tensor,tensor,out=z3) . tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]]) . sum = tensor.sum() print(sum.item(), type(sum.item())) . 12.0 &lt;class &#39;float&#39;&gt; . print(f&quot;{tensor} n&quot;) tensor.add_(5) # Just leave off the _ to stop this from being in-place! print(f&quot;{tensor}&quot;) . tensor([[6., 5., 6., 6.], [6., 5., 6., 6.], [6., 5., 6., 6.], [6., 5., 6., 6.]]) tensor([[11., 10., 11., 11.], [11., 10., 11., 11.], [11., 10., 11., 11.], [11., 10., 11., 11.]]) . NumPy bridging! . We can share the underlying memory between tensors and numpy arrays when on the cpu. . t = torch.ones(5) print(f&quot;t: {t}&quot;) n = t.numpy() print(f&quot;n: {n}&quot;) t[2] = 123 print(f&quot;t: {t}&quot;) print(f&quot;n: {n}&quot;) . t: tensor([1., 1., 1., 1., 1.]) n: [1. 1. 1. 1. 1.] t: tensor([ 1., 1., 123., 1., 1.]) n: [ 1. 1. 123. 1. 1.] . n = np.ones(5) print(f&quot;n: {n}&quot;) t = torch.from_numpy(n) print(f&quot;t: {t}&quot;) n[2] = 123 print(f&quot;n: {n}&quot;) print(f&quot;t: {t}&quot;) . n: [1. 1. 1. 1. 1.] t: tensor([1., 1., 1., 1., 1.], dtype=torch.float64) n: [ 1. 1. 123. 1. 1.] t: tensor([ 1., 1., 123., 1., 1.], dtype=torch.float64) .",
            "url": "https://wolfffff.github.io/fastpages/pytorch/jupyter/2022/04/30/pytorch_1.html",
            "relUrl": "/pytorch/jupyter/2022/04/30/pytorch_1.html",
            "date": " • Apr 30, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://wolfffff.github.io/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://wolfffff.github.io/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}