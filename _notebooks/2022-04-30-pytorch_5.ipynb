{"cells":[{"cell_type":"markdown","metadata":{},"source":["# PyTorch Tutorial - Part 5\n","> \"Part 5 of the PyTorch tutorial\"\n","\n","- toc: true\n","- branch: master\n","- badges: true\n","- comments: true\n","- author: Scott Wolf\n","- categories: [PyTorch, jupyter]"]},{"cell_type":"markdown","metadata":{"id":"HO-ys81DRx_q"},"source":["# Autograd\n","To train, we need access to the gradient of the loss function. To compute this gradient, PyTorch uses `torch.autograd` which can compute this gradient for any computational graph.\n","\n","Take the single layer neural network given here:"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":164,"status":"ok","timestamp":1651366377152,"user":{"displayName":"Scott Wolf","userId":"03447010338150970414"},"user_tz":240},"id":"Qn4EaDTaRj7U","outputId":"4bdb824a-0e68-40d1-e78d-99288a1d29ee"},"outputs":[{"data":{"text/plain":["tensor(0.5999, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","\n","x = torch.ones(5)  # input tensor\n","y = torch.zeros(3)  # expected output\n","w = torch.randn(5, 3, requires_grad=True)\n","b = torch.randn(3, requires_grad=True)\n","z = torch.matmul(x, w)+b\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n","loss"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":166,"status":"ok","timestamp":1651366432457,"user":{"displayName":"Scott Wolf","userId":"03447010338150970414"},"user_tz":240},"id":"1MSvE5CRSKrX","outputId":"09121c83-7701-4962-add4-3009481ac06e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Gradient function for z = <AddBackward0 object at 0x7f4122fa6650>\n","Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7f4122fa6150>\n"]}],"source":["print(f\"Gradient function for z = {z.grad_fn}\")\n","print(f\"Gradient function for loss = {loss.grad_fn}\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":160,"status":"ok","timestamp":1651366446084,"user":{"displayName":"Scott Wolf","userId":"03447010338150970414"},"user_tz":240},"id":"10CC06UXHAxv","outputId":"fcc37cc5-cb08-4d27-a818-7f00e80b8858"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.0142, 0.1601, 0.2225],\n","        [0.0142, 0.1601, 0.2225],\n","        [0.0142, 0.1601, 0.2225],\n","        [0.0142, 0.1601, 0.2225],\n","        [0.0142, 0.1601, 0.2225]])\n","tensor([0.0142, 0.1601, 0.2225])\n"]}],"source":["# Fills in grad -- remember that pytorch knows the relationship between x, w, b and z.\n","# It's just a computational graph!\n","loss.backward()\n","print(w.grad)\n","print(b.grad)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1651366675772,"user":{"displayName":"Scott Wolf","userId":"03447010338150970414"},"user_tz":240},"id":"kfiFULgVHEGW","outputId":"33f471b6-4fd1-4cd6-fa5b-a69fd9250caa"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","False\n","False\n"]}],"source":["#  Turn off requiring gradients! Saves lots of time on infer side.\n","z = torch.matmul(x, w)+b\n","print(z.requires_grad)\n","\n","with torch.no_grad():\n","    z = torch.matmul(x, w)+b\n","print(z.requires_grad)\n","\n","# We can also do \n","z_det = z.detach()\n","print(z_det.requires_grad)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6h9Cm8I3Hby-"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNJAfhYnGthB9VVBG3hLXBj","name":"pytorch_5.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
